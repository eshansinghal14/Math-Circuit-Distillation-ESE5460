# Circuit Distillation Requirements

# Core ML
torch>=2.1.0
transformers>=4.36.0
accelerate>=0.25.0

# AWS (optional, for cloud training)
boto3>=1.34.0

# Data processing
numpy>=1.24.0

# Utilities
tqdm>=4.66.0
huggingface_hub>=0.19.0
