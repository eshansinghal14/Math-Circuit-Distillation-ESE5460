%% LaTeX2e file `references.bib'
%% generated by the `filecontents' environment
%% from source `paper' on 2025/12/15.
%%
@article{dubey2024llama,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{wadhwa2025circuit,
  title={Circuit Distillation},
  author={Wadhwa, Somin and Amir, Silvio and Wallace, Byron C.},
  journal={arXiv preprint arXiv:2509.25002},
  year={2025}
}

@inproceedings{wadhwa2024mysteries,
  title={Investigating mysteries of CoT-augmented distillation},
  author={Wadhwa, Somin and Amir, Silvio and Wallace, Byron C},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={6071--6086},
  year={2024}
}

@article{mcgrath2023hydra,
  title={The Hydra Effect: Emergent Self-repair in Language Model Computations},
  author={McGrath, Thomas and Rahtz, Matthew and Kramar, Janos and Mikulik, Vladimir and Legg, Shane},
  journal={arXiv preprint arXiv:2307.15771},
  year={2023}
}

@misc{nanda2023attribution,
  title={Attribution Patching: Activation Patching At Industrial Scale},
  author={Nanda, Neel},
  howpublished={\url{[https://www.neelnanda.io/mechanistic-interpretability/attribution-patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)}},
  year={2023}
}

@article{kramar2024atp,
  title={AtP*: An efficient and scalable method for localizing LLM behaviour to components},
  author={Kram{\'a}r, J{\'a}nos and Lieberum, Tom and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2403.00745},
  year={2024}
}

@article{mueller2025mib,
  title={MIB: A Mechanistic Interpretability Benchmark},
  author={Mueller, Aaron and others},
  journal={arXiv preprint arXiv:2504.13151},
  year={2025}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of Neural Network Representations Revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@misc{sandoval2025fixing,
  title={Researchers fix Llama-3.1-8B reasoning errors with 8 even attention heads},
  author={Sandoval, Gustavo and others},
  howpublished={NYU Tandon School of Engineering News},
  year={2025}
}

@misc{nanda2022transformerlens,
  title={TransformerLens},
  author={Nanda, Neel and Bloom, Joseph},
  year={2022},
  howpublished={\url{[https://github.com/TransformerLensOrg/TransformerLens](https://github.com/TransformerLensOrg/TransformerLens)}}
}

@article{elhage2021mathematical,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
  journal={Transformer Circuits Thread},
  year={2021}
}

@article{williams2025equivalence,
  title={An Equivalence Between Representational Similarity Analysis and Centered Kernel Alignment},
  author={Williams, Alex and others},
  journal={Cognitive Computational Neuroscience},
  year={2025}
}
