\begin{thebibliography}{11}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, et~al.]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, et~al.]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and Hinton]{kornblith2019similarity}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Kram{\'a}r et~al.(2024)Kram{\'a}r, Lieberum, Shah, and Nanda]{kramar2024atp}
J{\'a}nos Kram{\'a}r, Tom Lieberum, Rohin Shah, and Neel Nanda.
\newblock Atp*: An efficient and scalable method for localizing llm behaviour to components.
\newblock \emph{arXiv preprint arXiv:2403.00745}, 2024.

\bibitem[Mueller et~al.(2025)]{mueller2025mib}
Aaron Mueller et~al.
\newblock Mib: A mechanistic interpretability benchmark.
\newblock \emph{arXiv preprint arXiv:2504.13151}, 2025.

\bibitem[Nanda(2023)]{nanda2023attribution}
Neel Nanda.
\newblock Attribution patching: Activation patching at industrial scale.
\newblock \url{[https://www.neelnanda.io/mechanistic-interpretability/attribution-patching](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)}, 2023.

\bibitem[Nanda and Bloom(2022)]{nanda2022transformerlens}
Neel Nanda and Joseph Bloom.
\newblock Transformerlens.
\newblock \url{[https://github.com/TransformerLensOrg/TransformerLens](https://github.com/TransformerLensOrg/TransformerLens)}, 2022.

\bibitem[Sandoval et~al.(2025)]{sandoval2025fixing}
Gustavo Sandoval et~al.
\newblock Researchers fix llama-3.1-8b reasoning errors with 8 even attention heads.
\newblock NYU Tandon School of Engineering News, 2025.

\bibitem[Wadhwa et~al.(2024)Wadhwa, Amir, and Wallace]{wadhwa2024mysteries}
Somin Wadhwa, Silvio Amir, and Byron~C Wallace.
\newblock Investigating mysteries of cot-augmented distillation.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 6071--6086, 2024.

\bibitem[Wadhwa et~al.(2025)Wadhwa, Amir, and Wallace]{wadhwa2025circuit}
Somin Wadhwa, Silvio Amir, and Byron~C. Wallace.
\newblock Circuit distillation.
\newblock \emph{arXiv preprint arXiv:2509.25002}, 2025.

\bibitem[Williams et~al.(2025)]{williams2025equivalence}
Alex Williams et~al.
\newblock An equivalence between representational similarity analysis and centered kernel alignment.
\newblock \emph{Cognitive Computational Neuroscience}, 2025.

\end{thebibliography}
